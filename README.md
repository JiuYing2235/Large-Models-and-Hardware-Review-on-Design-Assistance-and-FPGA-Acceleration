# Paper on FPGA-based LLM accelerator
This list focuses on understanding optimization methods for FPGA-based Large Language Model (LLM) accelerators.   
     
If you have any paper recommendations: Please contact me.  

## Papers
### 2025  
* [Design and Implementation of an FPGA-Based Hardware Accelerator for Transformer](http://arxiv.org/abs/2503.16731)
* [FÂ³: An FPGA-based Transformer Fine-tuning Accelerator with Flexible Floating Point Format](https://ieeexplore.ieee.org/document/10945317/)
* [FPGA Acceleration With Hessian-Based Comprehensive Intra-Layer Mixed-Precision Quantization for Transformer Models](https://ieeexplore.ieee.org/document/10973048/)
* [InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs](http://arxiv.org/abs/2502.08807)
### 2024
* [Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference](http://arxiv.org/abs/2312.15159)
* [FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer on UltraScale+ FPGAs](http://arxiv.org/abs/2409.14023)
* [A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs](http://arxiv.org/abs/2411.18148)
* [An FPGA-Based Transformer Accelerator With Parallel Unstructured Sparsity Handling for Question-Answering Applications](https://ieeexplore.ieee.org/document/10681589/)
* [Energy Efficient FPGA-Based Accelerator for Dynamic Sparse Transformer](https://ieeexplore.ieee.org/document/10652850/)
* [Energy Efficient FPGA-Based Binary Transformer Accelerator for Edge Devices](https://ieeexplore.ieee.org/document/10558631/)
* [FNM-Trans: Efficient FPGA-based Transformer Architecture with Full N:M Sparsity](https://dl.acm.org/doi/10.1145/3649329.3656497)
* [Enhancing Long Sequence Input Processing in FPGA-Based Transformer Accelerators through Attention Fusion](https://dl.acm.org/doi/10.1145/3649476.3658810)
* [MR-Transformer: FPGA Accelerated Deep Learning Attention Model for Modulation Recognition](https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html)
* [TransFRU: Efficient Deployment of Transformers on FPGA with Full Resource Utilization](https://ieeexplore.ieee.org/abstract/document/10473976)
* [ChatOPU: An FPGA-based Overlay Processor for Large Language Models with Unstructured Sparsity](https://dl.acm.org/doi/10.1145/3676536.3676761)
### 2023
* [A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks](https://dl.acm.org/doi/10.1145/3564606)
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* []()
* 
