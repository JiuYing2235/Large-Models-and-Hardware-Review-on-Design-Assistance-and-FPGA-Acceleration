# Paper on FPGA-based LLM accelerator
This list focuses on understanding optimization methods for FPGA-based Large Language Model (LLM) accelerators.   
     
If you have any paper recommendations: Please contact me.  

## Papers
### 2025  
* [Design and Implementation of an FPGA-Based Hardware Accelerator for Transformer](http://arxiv.org/abs/2503.16731)
* [FÂ³: An FPGA-based Transformer Fine-tuning Accelerator with Flexible Floating Point Format](https://ieeexplore.ieee.org/document/10945317/)
* [FPGA Acceleration With Hessian-Based Comprehensive Intra-Layer Mixed-Precision Quantization for Transformer Models](https://ieeexplore.ieee.org/document/10973048/)
* [InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs](http://arxiv.org/abs/2502.08807)
* [TerEffic: Highly Efficient Ternary LLM Inference on FPGA](http://arxiv.org/abs/2502.16473)
* [TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs](http://arxiv.org/abs/2504.16266)
* [Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for Efficient LLM Decoding on Embedded FPGA](https://ieeexplore.ieee.org/abstract/document/10993087)
* [On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration](http://arxiv.org/abs/2504.17376)
* [LoopLynx: A Scalable Dataflow Architecture for Efficient LLM Inference](https://ieeexplore.ieee.org/abstract/document/10993078)
* [LightMamba: Efficient Mamba Acceleration on FPGA with Quantization and Hardware Co-design](https://ieeexplore.ieee.org/abstract/document/10993079)
* [Hummingbird: A Smaller and Faster Large Language Model Accelerator on Embedded FPGA](http://arxiv.org/abs/2507.03308)
* [EdgeLLM: A Highly Efficient CPU-FPGA Heterogeneous Edge Accelerator for Large Language Models](https://ieeexplore.ieee.org/abstract/document/10916480)
* [AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design](http://arxiv.org/abs/2505.03745)
* [FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization](http://arxiv.org/abs/2505.18975)
### 2024
* [Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference](http://arxiv.org/abs/2312.15159)
* [FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer on UltraScale+ FPGAs](http://arxiv.org/abs/2409.14023)
* [A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs](http://arxiv.org/abs/2411.18148)
* [An FPGA-Based Transformer Accelerator With Parallel Unstructured Sparsity Handling for Question-Answering Applications](https://ieeexplore.ieee.org/document/10681589/)
* [Energy Efficient FPGA-Based Accelerator for Dynamic Sparse Transformer](https://ieeexplore.ieee.org/document/10652850/)
* [Energy Efficient FPGA-Based Binary Transformer Accelerator for Edge Devices](https://ieeexplore.ieee.org/document/10558631/)
* [FNM-Trans: Efficient FPGA-based Transformer Architecture with Full N:M Sparsity](https://dl.acm.org/doi/10.1145/3649329.3656497)
* [Enhancing Long Sequence Input Processing in FPGA-Based Transformer Accelerators through Attention Fusion](https://dl.acm.org/doi/10.1145/3649476.3658810)
* [MR-Transformer: FPGA Accelerated Deep Learning Attention Model for Modulation Recognition](https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html)
* [TransFRU: Efficient Deployment of Transformers on FPGA with Full Resource Utilization](https://ieeexplore.ieee.org/abstract/document/10473976)
* [ChatOPU: An FPGA-based Overlay Processor for Large Language Models with Unstructured Sparsity](https://dl.acm.org/doi/10.1145/3676536.3676761)
* [LlamaF: An Efficient Llama2 Architecture Accelerator on Embedded FPGAs](https://ieeexplore.ieee.org/abstract/document/10811385)
* [GLITCHES: GPU-FPGA LLM Inference Through a Collaborative Heterogeneous System](https://ieeexplore.ieee.org/abstract/document/10938498)
* [FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs](https://doi.org/10.1145/3626202.3637562)
* [Designing Efficient LLM Accelerators for Edge Devices](http://arxiv.org/abs/2408.00462)
* [Co-design of a TinyLLM using Programmable Logic and Software on an FPGA](https://ieeexplore.ieee.org/abstract/document/10658754)
* [Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference](https://doi.org/10.1145/3656177)
* [Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](http://arxiv.org/abs/2410.04466)
* [Hardware Acceleration of LLMs: A comprehensive survey and comparison](http://arxiv.org/abs/2409.03384)
### 2023
* [A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks](https://dl.acm.org/doi/10.1145/3564606)
* [FET-OPU: A Flexible and Efficient FPGA-Based Overlay Processor for Transformer Networks](https://ieeexplore.ieee.org/document/10323752/)
* [HPTA: A High Performance Transformer Accelerator Based on FPGA](https://ieeexplore.ieee.org/document/10296316/)
* [Transformer-OPU: An FPGA-based Overlay Processor for Transformer Networks](https://ieeexplore.ieee.org/abstract/document/10171578)
* [Unified Accelerator for Attention and Convolution in Inference Based on FPGA](https://ieeexplore.ieee.org/document/10182145/)
### 2022
* [Hardware Acceleration of Transformer Networks using FPGAs](https://ieeexplore.ieee.org/document/9976354/)
* [An Efficient Hardware Accelerator for Sparse Transformer Neural Networks](https://ieeexplore.ieee.org/document/9937659/)
* [A Length Adaptive Algorithm-Hardware Co-design of Transformer on FPGA Through Sparse Attention and Dynamic Pipelining](http://arxiv.org/abs/2208.03646)
* [An FPGA-Based Transformer Accelerator Using Output Block Stationary Dataflow for Object Recognition Applications](https://ieeexplore.ieee.org/document/9848824/)
* [TRAC: Compilation-Based Design of Transformer Accelerators for FPGAs](https://ieeexplore.ieee.org/document/10035242/)
* [DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation](https://ieeexplore.ieee.org/abstract/document/9923883)
### 2021
* [Accommodating Transformer onto FPGA: Coupling the Balanced Model Compression and FPGA-Implementation Optimization](https://dl.acm.org/doi/10.1145/3453688.3461739)
* [Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning](https://ieeexplore.ieee.org/document/9424344/)

