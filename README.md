# Paper on FPGA-based LLM accelerator
This list focuses on understanding optimization methods for FPGA-based Large Language Model (LLM) accelerators.   
     
If you have any paper recommendations: Please contact me.  

## Papers
### 2025  
* [Design and Implementation of an FPGA-Based Hardware Accelerator for Transformer](http://arxiv.org/abs/2503.16731)
* [FÂ³: An FPGA-based Transformer Fine-tuning Accelerator with Flexible Floating Point Format](https://ieeexplore.ieee.org/document/10945317/)
* [FPGA Acceleration With Hessian-Based Comprehensive Intra-Layer Mixed-Precision Quantization for Transformer Models](https://ieeexplore.ieee.org/document/10973048/)
* [InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs](http://arxiv.org/abs/2502.08807)
### 2024
* [Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference](http://arxiv.org/abs/2312.15159)
* [FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer on UltraScale+ FPGAs](http://arxiv.org/abs/2409.14023)
* [A Runtime-Adaptive Transformer Neural Network Accelerator on FPGAs](http://arxiv.org/abs/2411.18148)
* [An FPGA-Based Transformer Accelerator With Parallel Unstructured Sparsity Handling for Question-Answering Applications](https://ieeexplore.ieee.org/document/10681589/)
* [Energy Efficient FPGA-Based Accelerator for Dynamic Sparse Transformer](https://ieeexplore.ieee.org/document/10652850/)
* [Energy Efficient FPGA-Based Binary Transformer Accelerator for Edge Devices](https://ieeexplore.ieee.org/document/10558631/)
* []()
* []()
* []()
* []()
* []()
